---
title: "LMCache/LMCache: Supercharge Your LLM with the Fastest KV Cache Layer"
source: "https://github.com/LMCache/LMCache"
author:
  - "[[maobaolong]]"
  - "[[gemini-code-assist[bot]]]"
published:
created: 2025-10-01
description: "Supercharge Your LLM with the Fastest KV Cache Layer - LMCache/LMCache"
tags:
  - "clippings"
---
**[LMCache](https://github.com/LMCache/LMCache)** Public

Supercharge Your LLM with the Fastest KV Cache Layer

[lmcache.ai/](https://lmcache.ai/ "https://lmcache.ai/")

[Apache-2.0 license](https://github.com/LMCache/LMCache/blob/dev/LICENSE)

[Code of conduct](https://github.com/LMCache/LMCache/blob/dev/CODE_OF_CONDUCT.md)

[Contributing](https://github.com/LMCache/LMCache/blob/dev/CONTRIBUTING.md)

[Security policy](https://github.com/LMCache/LMCache/blob/dev/SECURITY.md)

[5.4k stars](https://github.com/LMCache/LMCache/stargazers) [616 forks](https://github.com/LMCache/LMCache/forks) [28 watching](https://github.com/LMCache/LMCache/watchers) [Branches](https://github.com/LMCache/LMCache/branches) [Tags](https://github.com/LMCache/LMCache/tags) [Activity](https://github.com/LMCache/LMCache/activity) [Custom properties](https://github.com/LMCache/LMCache/custom-properties)

Public repository

[Open in github.dev](https://github.dev/) [Open in a new github.dev tab](https://github.dev/) [Open in codespace](https://github.com/codespaces/new/LMCache/LMCache?resume=1)

<table><thead><tr><th colspan="2"><span>Name</span></th><th colspan="1"><span>Name</span></th><th><p><span>Last commit message</span></p></th><th colspan="1"><p><span>Last commit date</span></p></th></tr></thead><tbody><tr><td colspan="3"><p><span>and</span></p><p><span><a href="https://github.com/LMCache/LMCache/commit/81161f34408c7aebb79dbd6163acade7f444c843">[Core][RemoteBackend] Implement remove api for remote backend and fs_â€¦</a></span></p><p><span><a href="https://github.com/LMCache/LMCache/commit/81161f34408c7aebb79dbd6163acade7f444c843">81161f3</a> Â·</span></p><p><a href="https://github.com/LMCache/LMCache/commits/dev/"><span><span><span>789 Commits</span></span></span></a></p></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/.buildkite">.buildkite</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/.buildkite">.buildkite</a></p></td><td><p><a href="https://github.com/LMCache/LMCache/commit/8856379a64dd34c70fad87da54a65855ab009330">[CI/CD] Fix GDS test failure by adding use_direct_io = True (</a><a href="https://github.com/LMCache/LMCache/pull/1659">#1659</a><a href="https://github.com/LMCache/LMCache/commit/8856379a64dd34c70fad87da54a65855ab009330">)</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/.github">.github</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/.github">.github</a></p></td><td><p><a href="https://github.com/LMCache/LMCache/commit/d630f07e2390b23c68546be502c30162959a075f">[Test]: Enable unit tests to run on non-CUDA environment (</a><a href="https://github.com/LMCache/LMCache/pull/1575">#1575</a><a href="https://github.com/LMCache/LMCache/commit/d630f07e2390b23c68546be502c30162959a075f">)</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/asset">asset</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/asset">asset</a></p></td><td></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/benchmarks">benchmarks</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/benchmarks">benchmarks</a></p></td><td><p><a href="https://github.com/LMCache/LMCache/commit/4e5d036f816dfba695decf0ee9332dfacbcc0aab">[Benchmark] Emphasize query round results in long_doc_qa (</a><a href="https://github.com/LMCache/LMCache/pull/1730">#1730</a><a href="https://github.com/LMCache/LMCache/commit/4e5d036f816dfba695decf0ee9332dfacbcc0aab">)</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/csrc">csrc</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/csrc">csrc</a></p></td><td><p><a href="https://github.com/LMCache/LMCache/commit/a951f8d857a67da4067dc3c84bccc11c8643e403">[Enhancement] Add sparse attention for kv blending (</a><a href="https://github.com/LMCache/LMCache/pull/1414">#1414</a><a href="https://github.com/LMCache/LMCache/commit/a951f8d857a67da4067dc3c84bccc11c8643e403">)</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/docker">docker</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/docker">docker</a></p></td><td></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/docs">docs</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/docs">docs</a></p></td><td><p><a href="https://github.com/LMCache/LMCache/commit/7afee734982b8e3ab2e48c140e09ef91b1a61150">[Docs] Add minimal runnable quick start guide (</a><a href="https://github.com/LMCache/LMCache/pull/1725">#1725</a><a href="https://github.com/LMCache/LMCache/commit/7afee734982b8e3ab2e48c140e09ef91b1a61150">)</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/examples">examples</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/examples">examples</a></p></td><td></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/lmcache">lmcache</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/lmcache">lmcache</a></p></td><td><p><a href="https://github.com/LMCache/LMCache/commit/81161f34408c7aebb79dbd6163acade7f444c843">[Core][RemoteBackend] Implement remove api for remote backend and fs_â€¦</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/LMCache/LMCache/tree/dev/requirements">requirements</a></p></td><td colspan="1"><p><a href="https://github.com/LMCache/LMCache/tree/dev/requirements">requirements</a></p></td><td></td><td></td></tr><tr><td colspan="3"></td></tr></tbody></table>

[![lmcache logo](https://raw.githubusercontent.com/LMCache/LMCache/dev/asset/logo.png)](https://raw.githubusercontent.com/LMCache/LMCache/dev/asset/logo.png)

  

---

| [**Blog**](https://blog.lmcache.ai/) | [**Documentation**](https://docs.lmcache.ai/) | [**Join Slack**](https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-3bgx768yd-H8WkOTmPtbxVYJ5nuZ4dmA) | [**Interest Form**](https://forms.gle/MHwLiYDU6kcW3dLj7) | [**Roadmap**](https://github.com/LMCache/LMCache/issues/1253)

ðŸ”¥ **NEW: For enterprise-scale deployment of LMCache and vLLM, please check out vLLM [Production Stack](https://github.com/vllm-project/production-stack). LMCache is also officially supported in [llm-d](https://github.com/llm-d/llm-d/) and [KServe](https://github.com/kserve/kserve)!**

## Summary

LMCache is an **LLM** serving engine extension to **reduce TTFT** and **increase throughput**, especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of ***any*** reused text (not necessarily prefix) in ***any*** serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.

By combining LMCache with vLLM, developers achieve 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.

[![performance](https://private-user-images.githubusercontent.com/118159393/454068318-86137f17-f216-41a0-96a7-e537764f7a4c.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTkzMTE0OTQsIm5iZiI6MTc1OTMxMTE5NCwicGF0aCI6Ii8xMTgxNTkzOTMvNDU0MDY4MzE4LTg2MTM3ZjE3LWYyMTYtNDFhMC05NmE3LWU1Mzc3NjRmN2E0Yy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMDAxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTAwMVQwOTMzMTRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04Mjg0OTk4NGY0ZjI3YzIzN2FkZmExYTg2MzA2YTkxMWRmM2U0OGRiNmI5MjQwNGVlNmUxNzE0ZmRjMzUxNTNkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.KRvQ_xAoLybd3Nbru0McClkuVxJBCagy_PEMWBxU360)](https://private-user-images.githubusercontent.com/118159393/454068318-86137f17-f216-41a0-96a7-e537764f7a4c.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTkzMTE0OTQsIm5iZiI6MTc1OTMxMTE5NCwicGF0aCI6Ii8xMTgxNTkzOTMvNDU0MDY4MzE4LTg2MTM3ZjE3LWYyMTYtNDFhMC05NmE3LWU1Mzc3NjRmN2E0Yy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMDAxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTAwMVQwOTMzMTRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04Mjg0OTk4NGY0ZjI3YzIzN2FkZmExYTg2MzA2YTkxMWRmM2U0OGRiNmI5MjQwNGVlNmUxNzE0ZmRjMzUxNTNkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.KRvQ_xAoLybd3Nbru0McClkuVxJBCagy_PEMWBxU360)

## Features

- ðŸ”¥ Integration with vLLM v1 with the following features:
	- High performance CPU KVCache offloading
	- Disaggregated prefill
	- P2P KVCache sharing
- LMCache is supported in the [vLLM production stack](https://github.com/vllm-project/production-stack/), [llm-d](https://github.com/llm-d/llm-d/), and [KServe](https://github.com/kserve/kserve)
- Stable support for non-prefix KV caches
- Storage support as follows:
	- CPU
	- Disk
	- [NIXL](https://github.com/ai-dynamo/nixl)
- Installation support through pip and latest vLLM

## Installation

To use LMCache, simply install `lmcache` from your package manager, e.g. pip:

```
pip install lmcache
```

Works on Linux NVIDIA GPU platform.

More [detailed installation instructions](https://docs.lmcache.ai/getting_started/installation) are available in the docs, particularly if you are not using the latest stable version of vllm or using another serving engine with different dependencies. Any "undefined symbol" or torch mismatch versions can be resolved in the documentation.

## Getting started

The best way to get started is to checkout the [Quickstart Examples](https://docs.lmcache.ai/getting_started/quickstart/) in the docs.

## Documentation

Check out the LMCache [documentation](https://docs.lmcache.ai/) which is available online.

We also post regularly in [LMCache blogs](https://blog.lmcache.ai/).

## Examples

Go hands-on with our [examples](https://github.com/LMCache/LMCache/tree/dev/examples), demonstrating how to address different use cases with LMCache.

Fill out the [interest form](https://forms.gle/mQfQDUXbKfp2St1z7), [sign up for our newsletter](https://mailchi.mp/tensormesh/lmcache-sign-up-newsletter), [join LMCache slack](https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ), [check out LMCache website](https://lmcache.ai/), or [drop an email](https://github.com/LMCache/), and our team will reach out to you!

## Community meeting

The community meeting [Zoom Link](https://uchicago.zoom.us/j/6603596916?pwd=Z1E5MDRWUSt2am5XbEt4dTFkNGx6QT09) for LMCache is hosted bi-weekly. All are welcome to join!

Meetings are held bi-weekly on: Tuesdays at 9:00 AM PT â€“ [Add to Google Calendar](https://calendar.google.com/calendar/u/0/r?cid=Y19mNGY2ZmMwZjUxMWYyYTZmZmE1ZTVlMGI2Yzk2NmFmZjNhM2Y4ODZiZmU5OTU5MDJlMmE3ZmUyOGZmZThlOWY5QGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20)

We keep notes from each meeting on this [document](https://docs.google.com/document/d/1_Fl3vLtERFa3vTH00cezri78NihNBtSClK-_1tSrcow) for summaries of standups, discussion, and action items.

Recordings of meetings are available on the [YouTube LMCache channel](https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA).

## Contributing

We welcome and value all contributions and collaborations. Please check out [Contributing Guide](https://github.com/LMCache/LMCache/blob/dev/CONTRIBUTING.md) on how to contribute.

We continually update [\[Onboarding\] Welcoming contributors with good first issues!](https://github.com/LMCache/LMCache/issues/627)

## Citation

If you use LMCache for your research, please cite our papers:

```
@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cheng2024large,
  title={Do Large Language Models Need a Content Delivery Network?},
  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},
  journal={arXiv preprint arXiv:2409.13761},
  year={2024}
}

@inproceedings{10.1145/3689031.3696098,
  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},
  year = {2025},
  url = {https://doi.org/10.1145/3689031.3696098},
  doi = {10.1145/3689031.3696098},
  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
  pages = {94â€“109},
}
```

## Socials

[Linkedin](https://www.linkedin.com/company/lmcache-lab/?viewAsMember=true) | [Twitter](https://x.com/lmcache) | [Youtube](https://www.youtube.com/@LMCacheTeam)

## License

The LMCache codebase is licensed under Apache License 2.0. See the [LICENSE](https://github.com/LMCache/LMCache/blob/dev/LICENSE) file for details.

## Releases 16

[\+ 15 releases](https://github.com/LMCache/LMCache/releases)

## Packages

No packages published  

## Languages

- [Python 93.4%](https://github.com/LMCache/LMCache/search?l=python)
- [Cuda 4.4%](https://github.com/LMCache/LMCache/search?l=cuda)
- [Shell 1.5%](https://github.com/LMCache/LMCache/search?l=shell)
- Other 0.7%