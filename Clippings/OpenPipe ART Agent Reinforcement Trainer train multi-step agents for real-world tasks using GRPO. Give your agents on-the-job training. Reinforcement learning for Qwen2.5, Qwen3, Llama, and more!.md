---
title: "OpenPipe/ART: Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!"
source: https://github.com/OpenPipe/ART
author:
  - "[[bradhilton]]"
published:
created: 2025-09-30
description: "Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more! - OpenPipe/ART"
tags:
  - clippings
  - agent
---
**[ART](https://github.com/OpenPipe/ART)** Public

Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!

[art.openpipe.ai](https://art.openpipe.ai/ "https://art.openpipe.ai")

[Apache-2.0 license](https://github.com/OpenPipe/ART/blob/main/LICENSE)

[Open in github.dev](https://github.dev/) [Open in a new github.dev tab](https://github.dev/) [Open in codespace](https://github.com/codespaces/new/OpenPipe/ART?resume=1)

<table><thead><tr><th colspan="2"><span>Name</span></th><th colspan="1"><span>Name</span></th><th><p><span>Last commit message</span></p></th><th colspan="1"><p><span>Last commit date</span></p></th></tr></thead><tbody><tr><td colspan="3"><p><span><a href="https://github.com/OpenPipe/ART/commit/3c144bd18ef4675ae707593861250dced2a333c7">chore: Update client.py &amp; backend.py</a></span></p><p><span><a href="https://github.com/OpenPipe/ART/commit/3c144bd18ef4675ae707593861250dced2a333c7">3c144bd</a> ¬∑</span></p><p><a href="https://github.com/OpenPipe/ART/commits/main/"><span><span><span>974 Commits</span></span></span></a></p></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/.github/workflows"><span>.github/</span> <span>workflows</span></a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/.github/workflows"><span>.github/</span> <span>workflows</span></a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/677d286169055e14c8db7f9836363bf0219b8c72">feat: Add experimental standard deviation learning rate scheduling (</a><a href="https://github.com/OpenPipe/ART/pull/340">#340</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/assets">assets</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/assets">assets</a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/0e1f70ef7cac05d5e0ec46ff05c5882d1edef01d">Update notebooks markdown and upload code (</a><a href="https://github.com/OpenPipe/ART/pull/356">#356</a><a href="https://github.com/OpenPipe/ART/commit/0e1f70ef7cac05d5e0ec46ff05c5882d1edef01d">)</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/dev">dev</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/dev">dev</a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/2e549b0532f8d3b0cb547c750037ca701d599a10">chore: Update yes-no-maybe.ipynb</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/docs">docs</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/docs">docs</a></p></td><td></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/examples">examples</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/examples">examples</a></p></td><td></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/licenses">licenses</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/licenses">licenses</a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/1ff733f9dac27b16a9875f02b0ac0f9ef769f841">chore: Update THIRD-PARTY-NOTICES and notebooks to include Unsloth ad‚Ä¶</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/requirements">requirements</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/requirements">requirements</a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/f7db51fd8100a0a9af6a365ced4f5ddb06408d23">pypi: remove direct git deps from metadata; add requirements/backend.‚Ä¶</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/scripts">scripts</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/scripts">scripts</a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/1a0695559c739075b2d07033e2cb90783ceecb30">fix: Update run_checks.sh to include tests in pyright command and ref‚Ä¶</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/src">src</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/src">src</a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/3c144bd18ef4675ae707593861250dced2a333c7">chore: Update client.py &amp; backend.py</a></p></td><td></td></tr><tr><td colspan="2"><p><a href="https://github.com/OpenPipe/ART/tree/main/tests">tests</a></p></td><td colspan="1"><p><a href="https://github.com/OpenPipe/ART/tree/main/tests">tests</a></p></td><td><p><a href="https://github.com/OpenPipe/ART/commit/d7ea95236f87e97c1d2520b118707adfeaf7e186">feat: Auto trajectory</a></p></td><td></td></tr><tr><td colspan="3"></td></tr></tbody></table>

**RULER** (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the rest‚Äî **no labeled data, expert feedback, or reward engineering required**.

‚ú® **Key Benefits:**

- **2-3x faster development** - Skip reward function engineering entirely
- **General-purpose** - Works across any task without modification
- **Strong performance** - Matches or exceeds hand-crafted rewards in 3/4 benchmarks
- **Easy integration** - Drop-in replacement for manual reward functions
```
# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, "openai/o3")
```

[üìñ Learn more about RULER ‚Üí](https://art.openpipe.ai/fundamentals/ruler)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the [docs](https://art.openpipe.ai/).

## üìí Notebooks

| Agent Task | Example Notebook | Description | Comparative Performance |
| --- | --- | --- | --- |
| **ART‚Ä¢E LangGraph** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb) | Qwen 2.5 7B learns to search emails using LangGraph | \[Link coming soon\] |
| **MCP‚Ä¢RL** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb) | Qwen 2.5 3B masters the NWS MCP server | \[Link coming soon\] |
| **ART‚Ä¢E \[RULER\]** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb) | Qwen 2.5 7B learns to search emails using RULER | [![](https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg)](https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg) [benchmarks](https://github.com/OpenPipe/ART/blob/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb) |
| **2048** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb) | Qwen 2.5 3B learns to play 2048 | [![](https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg)](https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg) [benchmarks](https://github.com/OpenPipe/ART/blob/main/examples/2048/display_benchmarks.ipynb) |
| **Temporal Clue** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue | \[Link coming soon\] |
| **Tic Tac Toe** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb) | Qwen 2.5 3B learns to play Tic Tac Toe | [![](https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg)](https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg) [benchmarks](https://github.com/OpenPipe/ART/blob/main/examples/tic_tac_toe/display-benchmarks.ipynb) |
| **Codenames** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb) | Qwen 2.5 3B learns to play Codenames | [![](https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png)](https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png) [benchmarks](https://github.com/OpenPipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb) |
| **AutoRL \[RULER\]** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb) | Train Qwen 2.5 7B to master any task | \[Link coming soon\] |

Explore our latest research and updates on building SOTA agents.

- üóûÔ∏è **[ART now integrates seamlessly with LangGraph](https://art.openpipe.ai/integrations/langgraph-integration)** - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.
- üóûÔ∏è **[MCP‚Ä¢RL: Teach Your Model to Master Any MCP Server](https://x.com/corbtt/status/1953171838382817625)** - Automatically train models to effectively use MCP server tools through reinforcement learning.
- üóûÔ∏è **[AutoRL: Zero-Data Training for Any Task](https://x.com/mattshumer_/status/1950572449025650733)** - Train custom AI models without labeled data using automatic input generation and RULER evaluation.
- üóûÔ∏è **[RULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards)** is now available for automatic reward generation in reinforcement learning.
- üóûÔ∏è **[ART¬∑E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)** demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.
- üóûÔ∏è **[ART Trainer: A New RL Trainer for Agents](https://openpipe.ai/blog/art-trainer)** enables easy training of LLM-based agents using GRPO.

[üìñ See all blog posts ‚Üí](https://openpipe.ai/blog)

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn't need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

Curious about how to use ART for a real-world task? Check out the [ART‚Ä¢E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

[![](https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png)](https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png)

ART's functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**
	1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
	2. Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.
	3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
	4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.
2. **Training**
	1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
	2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
	3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
	4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## ü§ù Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](https://github.com/OpenPipe/ART/blob/main/CONTRIBUTING.md) file for more information.

## üìñ Citation

```
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## ‚öñÔ∏è License

This repository's source code is available under the [Apache-2.0 License](https://github.com/OpenPipe/ART/blob/main/LICENSE).

## üôè Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)
- [SkyPilot](https://github.com/skypilot-org/skypilot)

Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.

## Releases 41

[\+ 40 releases](https://github.com/OpenPipe/ART/releases)

## Packages

No packages published  

## Deployments 77

- [staging - docs](https://github.com/OpenPipe/ART/deployments/staging%20-%20docs)

[\+ 76 deployments](https://github.com/OpenPipe/ART/deployments)

## Languages

- [Python 88.7%](https://github.com/OpenPipe/ART/search?l=python)
- [Jupyter Notebook 10.5%](https://github.com/OpenPipe/ART/search?l=jupyter-notebook)
- [Shell 0.8%](https://github.com/OpenPipe/ART/search?l=shell)