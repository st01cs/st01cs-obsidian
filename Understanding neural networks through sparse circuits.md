
  
OpenAI最新发布了一款“权重稀疏”大型语言模型（LLM），首次让我们能够真正看清AI内部是如何工作的。这款模型虽小，性能相当于2018年的GPT-1，但其最大价值在于极大提升了模型的可解释性。  
  
传统LLM如GPT-4、Claude等，是典型的“黑箱”，成千上万个神经元紧密连接，决策路径混乱难解，专家也难以准确理解其推理过程。这导致模型容易产生幻觉、行为不稳定、判断失误，尤其在关键场景下风险巨大。  
  
这次的稀疏Transformer通过强制绝大多数连接权重为零，每个神经元只保留几十个连接，技能模块分开走独立路径，使得模型内部形成“简洁电路”。研发团队能明确定位支撑某一行为的“最小回路”，验证其必要性和充分性，真正做到能命名、绘制、干预和测试。  
  
这套训练方法不仅让机制变得清晰，也推动了模型规模和可解释性的同步提升。虽然复杂技能如代码变量绑定的回路还未完全绘制，但部分机制已经能够准确预测模型行为。  
  
这不只是为了打造更小的模型，而是开辟了研究更复杂系统内部机理的道路。未来，AI不应仅仅是强大黑箱，而应成为我们能理解、控制、信赖的工具。  
  
这或许是AI可控性迈出的关键一步。透过“稀疏回路”，我们看到了“隐秘的思维路径”，剥开了统计学迷雾，朝着“可靠智能”前进。  
  
如果AI是未来的引擎，那么理解它的“线路图”比追求更大规模更为重要。唯有如此，AI才能真正为社会带来信任和安全，而非神秘与未知的风险。  
  
原文链接：openai.com/index/understanding-neural-networks-through-sparse-circuits/